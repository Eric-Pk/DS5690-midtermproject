# Extending Context Window of Large Language Models via Position Interpolation

## Authors

- Shouyuan Chen
- Sherman Wong
- Liangjian Chen
- Yuandong Tian

**Affiliation**: Meta Platforms Inc.

## Overview

Position Interpolation (PI) extends the context window sizes of RoPE-based pretrained LLMs. The problem addressed is the limitation of context window sizes in existing models, and the solution presented is the introduction of Position Interpolation to overcome this limitation. The approach demonstrates strong empirical results on various tasks requiring long context.

## Architecture Overview

<!-- Placeholder for the pseudocode description of the proposed model and differences from previous models -->

## Critical Analysis

<!-- Placeholder for the analysis, addressing questions like what was overlooked, errors, potential improvements, and disputes -->

## Questions for Discussion

1. Question 1: <!-- Placeholder for the first chosen topic and question for discussion -->
2. Question 2: <!-- Placeholder for the second chosen topic and question for discussion -->

## Resources

- [Link to another relevant paper](#)
- [Link to model's official page](#)
- [Link to a related blog post](#)
- [PapersWithCode link](#)
- <!-- Add more links as needed -->

## Code Demonstration

See the attached Jupyter Notebook for a demonstration of using the proposed model/approach.

## Repository Contents

- README.md - This file
- Notebook.ipynb - Jupyter notebook demonstrating the model
- video_link.txt - Link to the video recording of the overview

## Video Recording

[Link to the video recording of the overview](#)

## Citation

```
@inproceedings{...}  <!-- Placeholder for the actual BibTeX citation -->
```
